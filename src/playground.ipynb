{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images input\n",
      "torch.Size([2, 1, 6, 6])\n",
      "tensor([[[[0.5463, 0.7340, 0.0529, 0.3832, 0.0270, 0.0762],\n",
      "          [0.8616, 0.4619, 0.0695, 0.2323, 0.3086, 0.1032],\n",
      "          [0.4636, 0.2489, 0.1030, 0.5925, 0.8976, 0.3730],\n",
      "          [0.1945, 0.5421, 0.6404, 0.4513, 0.6649, 0.8863],\n",
      "          [0.7978, 0.1297, 0.7143, 0.4763, 0.7851, 0.6691],\n",
      "          [0.2878, 0.9141, 0.4255, 0.4009, 0.6997, 0.7527]]],\n",
      "\n",
      "\n",
      "        [[[0.6216, 0.5607, 0.9803, 0.7456, 0.4444, 0.6561],\n",
      "          [0.1483, 0.4778, 0.0693, 0.7137, 0.3872, 0.3660],\n",
      "          [0.8805, 0.1232, 0.2228, 0.4856, 0.0156, 0.6362],\n",
      "          [0.8774, 0.1551, 0.5922, 0.4736, 0.5383, 0.8704],\n",
      "          [0.8192, 0.9175, 0.8558, 0.5901, 0.9186, 0.8109],\n",
      "          [0.0681, 0.9770, 0.4072, 0.3955, 0.9965, 0.8976]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 1\n",
    "height, width = 6,6\n",
    "images = torch.rand(batch_size, in_channels, height, width)\n",
    "print(\"Images input\")\n",
    "print(images.shape)\n",
    "print(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im2col result\n",
      "torch.Size([2, 9, 16])\n",
      "tensor([[[0.5463, 0.7340, 0.0529, 0.3832, 0.8616, 0.4619, 0.0695, 0.2323,\n",
      "          0.4636, 0.2489, 0.1030, 0.5925, 0.1945, 0.5421, 0.6404, 0.4513],\n",
      "         [0.7340, 0.0529, 0.3832, 0.0270, 0.4619, 0.0695, 0.2323, 0.3086,\n",
      "          0.2489, 0.1030, 0.5925, 0.8976, 0.5421, 0.6404, 0.4513, 0.6649],\n",
      "         [0.0529, 0.3832, 0.0270, 0.0762, 0.0695, 0.2323, 0.3086, 0.1032,\n",
      "          0.1030, 0.5925, 0.8976, 0.3730, 0.6404, 0.4513, 0.6649, 0.8863],\n",
      "         [0.8616, 0.4619, 0.0695, 0.2323, 0.4636, 0.2489, 0.1030, 0.5925,\n",
      "          0.1945, 0.5421, 0.6404, 0.4513, 0.7978, 0.1297, 0.7143, 0.4763],\n",
      "         [0.4619, 0.0695, 0.2323, 0.3086, 0.2489, 0.1030, 0.5925, 0.8976,\n",
      "          0.5421, 0.6404, 0.4513, 0.6649, 0.1297, 0.7143, 0.4763, 0.7851],\n",
      "         [0.0695, 0.2323, 0.3086, 0.1032, 0.1030, 0.5925, 0.8976, 0.3730,\n",
      "          0.6404, 0.4513, 0.6649, 0.8863, 0.7143, 0.4763, 0.7851, 0.6691],\n",
      "         [0.4636, 0.2489, 0.1030, 0.5925, 0.1945, 0.5421, 0.6404, 0.4513,\n",
      "          0.7978, 0.1297, 0.7143, 0.4763, 0.2878, 0.9141, 0.4255, 0.4009],\n",
      "         [0.2489, 0.1030, 0.5925, 0.8976, 0.5421, 0.6404, 0.4513, 0.6649,\n",
      "          0.1297, 0.7143, 0.4763, 0.7851, 0.9141, 0.4255, 0.4009, 0.6997],\n",
      "         [0.1030, 0.5925, 0.8976, 0.3730, 0.6404, 0.4513, 0.6649, 0.8863,\n",
      "          0.7143, 0.4763, 0.7851, 0.6691, 0.4255, 0.4009, 0.6997, 0.7527]],\n",
      "\n",
      "        [[0.6216, 0.5607, 0.9803, 0.7456, 0.1483, 0.4778, 0.0693, 0.7137,\n",
      "          0.8805, 0.1232, 0.2228, 0.4856, 0.8774, 0.1551, 0.5922, 0.4736],\n",
      "         [0.5607, 0.9803, 0.7456, 0.4444, 0.4778, 0.0693, 0.7137, 0.3872,\n",
      "          0.1232, 0.2228, 0.4856, 0.0156, 0.1551, 0.5922, 0.4736, 0.5383],\n",
      "         [0.9803, 0.7456, 0.4444, 0.6561, 0.0693, 0.7137, 0.3872, 0.3660,\n",
      "          0.2228, 0.4856, 0.0156, 0.6362, 0.5922, 0.4736, 0.5383, 0.8704],\n",
      "         [0.1483, 0.4778, 0.0693, 0.7137, 0.8805, 0.1232, 0.2228, 0.4856,\n",
      "          0.8774, 0.1551, 0.5922, 0.4736, 0.8192, 0.9175, 0.8558, 0.5901],\n",
      "         [0.4778, 0.0693, 0.7137, 0.3872, 0.1232, 0.2228, 0.4856, 0.0156,\n",
      "          0.1551, 0.5922, 0.4736, 0.5383, 0.9175, 0.8558, 0.5901, 0.9186],\n",
      "         [0.0693, 0.7137, 0.3872, 0.3660, 0.2228, 0.4856, 0.0156, 0.6362,\n",
      "          0.5922, 0.4736, 0.5383, 0.8704, 0.8558, 0.5901, 0.9186, 0.8109],\n",
      "         [0.8805, 0.1232, 0.2228, 0.4856, 0.8774, 0.1551, 0.5922, 0.4736,\n",
      "          0.8192, 0.9175, 0.8558, 0.5901, 0.0681, 0.9770, 0.4072, 0.3955],\n",
      "         [0.1232, 0.2228, 0.4856, 0.0156, 0.1551, 0.5922, 0.4736, 0.5383,\n",
      "          0.9175, 0.8558, 0.5901, 0.9186, 0.9770, 0.4072, 0.3955, 0.9965],\n",
      "         [0.2228, 0.4856, 0.0156, 0.6362, 0.5922, 0.4736, 0.5383, 0.8704,\n",
      "          0.8558, 0.5901, 0.9186, 0.8109, 0.4072, 0.3955, 0.9965, 0.8976]]])\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 0\n",
    "im2col_result = F.unfold(images, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "print(\"im2col result\")\n",
    "print(im2col_result.shape)\n",
    "print(im2col_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernels Flattened\n",
      "torch.Size([4, 9])\n",
      "tensor([[ 0.0903,  1.6831, -1.2517,  1.4519, -0.4311, -1.9387, -0.5444,  0.6553,\n",
      "         -0.7051],\n",
      "        [ 0.2976,  0.4306,  2.2662,  1.4804, -0.6277, -0.7304,  0.8510,  1.0613,\n",
      "         -0.4742],\n",
      "        [ 0.3410,  2.0507, -1.1369, -0.2150,  0.6438,  1.4654, -0.1203, -0.1643,\n",
      "          0.8366],\n",
      "        [ 0.3783, -1.2285, -0.2971,  1.1674,  0.0966,  1.3630, -0.2323,  1.2406,\n",
      "          2.0313]])\n"
     ]
    }
   ],
   "source": [
    "out_channels = 4\n",
    "conv_kernel = torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "kernel_matrix = conv_kernel.view(out_channels, -1)\n",
    "print(\"Kernels Flattened\")\n",
    "print(kernel_matrix.shape)\n",
    "print(kernel_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution using im2col (Unfold)\n",
      "torch.Size([2, 4, 16])\n",
      "tensor([[[ 1.9738, -0.6198, -0.2823, -0.0087,  0.9321, -1.1576, -2.3568,\n",
      "          -0.2736, -1.7140, -0.8481, -1.3011, -0.4691, -0.0118, -0.9827,\n",
      "          -1.1674, -1.1842],\n",
      "         [ 2.1431,  1.6201,  0.2644,  1.6531,  1.5048,  1.4917,  0.6536,\n",
      "           1.1467,  0.4364,  2.1749,  3.2402,  1.9328,  3.3337,  1.8950,\n",
      "           2.5326,  2.8796],\n",
      "         [ 1.8347,  0.6577,  2.0010,  0.4927,  1.7967,  1.1241,  2.2288,\n",
      "           2.1696,  2.2772,  0.8449,  1.8495,  3.6219,  1.5799,  2.2705,\n",
      "           2.1596,  2.3598],\n",
      "         [ 0.8447,  2.2349,  2.5997,  2.2643,  2.3717,  2.7137,  2.8119,\n",
      "           3.4853,  2.4180,  2.9250,  2.7615,  3.0323,  3.0663,  1.2839,\n",
      "           3.2601,  2.9384]],\n",
      "\n",
      "        [[-0.9080, -0.2159,  0.0155, -0.5491,  0.7306, -1.6226,  0.4152,\n",
      "          -0.7958, -0.3812, -1.5248, -0.2970, -2.2492, -0.9501, -0.3069,\n",
      "          -1.2811, -1.4474],\n",
      "         [ 3.2915,  2.5324,  1.6893,  2.5744,  2.1009,  2.0130,  1.9705,\n",
      "           2.0148,  2.8543,  2.1543,  1.4158,  2.4130,  2.5834,  2.8407,\n",
      "           2.1194,  3.0185],\n",
      "         [ 0.6847,  2.6963,  2.2766,  1.5230,  1.5326,  0.6025,  1.6358,\n",
      "           2.0419,  1.5451,  1.2315,  2.5892,  1.4510,  1.7844,  2.0935,\n",
      "           2.8228,  2.4682],\n",
      "         [-0.0302,  1.5578,  0.5827,  2.1095,  1.9835,  2.3715,  0.9061,\n",
      "           3.4470,  4.6482,  2.5595,  3.3530,  4.4166,  4.2003,  2.2301,\n",
      "           4.2105,  4.1097]]])\n"
     ]
    }
   ],
   "source": [
    "im2col_conv2d_result = torch.matmul(kernel_matrix, im2col_result)\n",
    "print(\"Convolution using im2col (Unfold)\")\n",
    "print(im2col_conv2d_result.shape)\n",
    "print(im2col_conv2d_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape im2col multiplication result into output feature map\n",
      "torch.Size([2, 4, 4, 4])\n",
      "tensor([[[[ 1.9738, -0.6198, -0.2823, -0.0087],\n",
      "          [ 0.9321, -1.1576, -2.3568, -0.2736],\n",
      "          [-1.7140, -0.8481, -1.3011, -0.4691],\n",
      "          [-0.0118, -0.9827, -1.1674, -1.1842]],\n",
      "\n",
      "         [[ 2.1431,  1.6201,  0.2644,  1.6531],\n",
      "          [ 1.5048,  1.4917,  0.6536,  1.1467],\n",
      "          [ 0.4364,  2.1749,  3.2402,  1.9328],\n",
      "          [ 3.3337,  1.8950,  2.5326,  2.8796]],\n",
      "\n",
      "         [[ 1.8347,  0.6577,  2.0010,  0.4927],\n",
      "          [ 1.7967,  1.1241,  2.2288,  2.1696],\n",
      "          [ 2.2772,  0.8449,  1.8495,  3.6219],\n",
      "          [ 1.5799,  2.2705,  2.1596,  2.3598]],\n",
      "\n",
      "         [[ 0.8447,  2.2349,  2.5997,  2.2643],\n",
      "          [ 2.3717,  2.7137,  2.8119,  3.4853],\n",
      "          [ 2.4180,  2.9250,  2.7615,  3.0323],\n",
      "          [ 3.0663,  1.2839,  3.2601,  2.9384]]],\n",
      "\n",
      "\n",
      "        [[[-0.9080, -0.2159,  0.0155, -0.5491],\n",
      "          [ 0.7306, -1.6226,  0.4152, -0.7958],\n",
      "          [-0.3812, -1.5248, -0.2970, -2.2492],\n",
      "          [-0.9501, -0.3069, -1.2811, -1.4474]],\n",
      "\n",
      "         [[ 3.2915,  2.5324,  1.6893,  2.5744],\n",
      "          [ 2.1009,  2.0130,  1.9705,  2.0148],\n",
      "          [ 2.8543,  2.1543,  1.4158,  2.4130],\n",
      "          [ 2.5834,  2.8407,  2.1194,  3.0185]],\n",
      "\n",
      "         [[ 0.6847,  2.6963,  2.2766,  1.5230],\n",
      "          [ 1.5326,  0.6025,  1.6358,  2.0419],\n",
      "          [ 1.5451,  1.2315,  2.5892,  1.4510],\n",
      "          [ 1.7844,  2.0935,  2.8228,  2.4682]],\n",
      "\n",
      "         [[-0.0302,  1.5578,  0.5827,  2.1095],\n",
      "          [ 1.9835,  2.3715,  0.9061,  3.4470],\n",
      "          [ 4.6482,  2.5595,  3.3530,  4.4166],\n",
      "          [ 4.2003,  2.2301,  4.2105,  4.1097]]]])\n"
     ]
    }
   ],
   "source": [
    "out_height = (height - kernel_size) // stride + 1\n",
    "out_width = (width - kernel_size) // stride + 1\n",
    "output_feature_map = im2col_conv2d_result.view(batch_size, out_channels, out_height, out_width)\n",
    "print(\"Reshape im2col multiplication result into output feature map\")\n",
    "print(output_feature_map.shape)\n",
    "print(output_feature_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution2d call result\n",
      "torch.Size([2, 4, 4, 4])\n",
      "tensor([[[[ 1.9738, -0.6198, -0.2823, -0.0087],\n",
      "          [ 0.9321, -1.1576, -2.3568, -0.2736],\n",
      "          [-1.7140, -0.8481, -1.3011, -0.4691],\n",
      "          [-0.0118, -0.9827, -1.1674, -1.1842]],\n",
      "\n",
      "         [[ 2.1431,  1.6201,  0.2644,  1.6531],\n",
      "          [ 1.5048,  1.4917,  0.6536,  1.1467],\n",
      "          [ 0.4364,  2.1749,  3.2402,  1.9328],\n",
      "          [ 3.3337,  1.8950,  2.5326,  2.8796]],\n",
      "\n",
      "         [[ 1.8347,  0.6577,  2.0010,  0.4927],\n",
      "          [ 1.7967,  1.1241,  2.2288,  2.1696],\n",
      "          [ 2.2772,  0.8449,  1.8495,  3.6219],\n",
      "          [ 1.5799,  2.2705,  2.1596,  2.3598]],\n",
      "\n",
      "         [[ 0.8447,  2.2349,  2.5997,  2.2643],\n",
      "          [ 2.3717,  2.7137,  2.8119,  3.4853],\n",
      "          [ 2.4180,  2.9250,  2.7615,  3.0323],\n",
      "          [ 3.0663,  1.2839,  3.2601,  2.9384]]],\n",
      "\n",
      "\n",
      "        [[[-0.9080, -0.2159,  0.0155, -0.5491],\n",
      "          [ 0.7306, -1.6226,  0.4152, -0.7958],\n",
      "          [-0.3812, -1.5248, -0.2970, -2.2492],\n",
      "          [-0.9501, -0.3069, -1.2811, -1.4474]],\n",
      "\n",
      "         [[ 3.2915,  2.5324,  1.6893,  2.5744],\n",
      "          [ 2.1009,  2.0130,  1.9705,  2.0148],\n",
      "          [ 2.8543,  2.1543,  1.4158,  2.4130],\n",
      "          [ 2.5834,  2.8407,  2.1194,  3.0185]],\n",
      "\n",
      "         [[ 0.6847,  2.6963,  2.2766,  1.5230],\n",
      "          [ 1.5326,  0.6025,  1.6358,  2.0419],\n",
      "          [ 1.5451,  1.2315,  2.5892,  1.4510],\n",
      "          [ 1.7844,  2.0935,  2.8228,  2.4682]],\n",
      "\n",
      "         [[-0.0302,  1.5578,  0.5827,  2.1095],\n",
      "          [ 1.9835,  2.3715,  0.9061,  3.4470],\n",
      "          [ 4.6482,  2.5595,  3.3530,  4.4166],\n",
      "          [ 4.2003,  2.2301,  4.2105,  4.1097]]]])\n"
     ]
    }
   ],
   "source": [
    "conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
    "conv_layer.weight.data = conv_kernel\n",
    "\n",
    "conv2d_result = conv_layer(images)\n",
    "conv2d_no_grad = conv2d_result.detach()\n",
    "print(\"Convolution2d call result\")\n",
    "print(conv2d_no_grad.shape)\n",
    "print(conv2d_no_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "errors = conv2d_no_grad - output_feature_map\n",
    "print(errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
