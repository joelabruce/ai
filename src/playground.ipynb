{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images input\n",
      "torch.Size([2, 1, 5, 5])\n",
      "tensor([[[[-1.4810, -0.4897, -1.8688, -0.8012,  1.3305],\n",
      "          [ 0.8573,  0.5150, -0.0738, -1.3466,  1.1518],\n",
      "          [ 0.2045, -0.7090,  0.4457, -1.2756, -0.7034],\n",
      "          [-0.3336, -0.4023,  2.3146,  0.1836, -0.8412],\n",
      "          [ 0.9116,  1.2257, -1.0256, -0.3944, -0.5429]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5863, -0.1345, -0.3005,  0.0024, -0.3322],\n",
      "          [-0.2621,  0.9806,  0.1233, -0.8503, -0.3742],\n",
      "          [ 1.2416, -1.0087, -0.1818, -0.3855,  0.9696],\n",
      "          [-0.7441, -0.4278,  0.1171,  1.1782, -1.6178],\n",
      "          [-0.7069, -0.5989, -0.0132,  0.7005,  0.7016]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 2\n",
    "in_channels = 1\n",
    "height, width = 5, 5\n",
    "images = torch.randn(batch_size, in_channels, height, width)\n",
    "print(\"Images input\")\n",
    "print(images.shape)\n",
    "print(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im2col result\n",
      "torch.Size([2, 9, 9])\n",
      "tensor([[[-1.4810, -0.4897, -1.8688,  0.8573,  0.5150, -0.0738,  0.2045,\n",
      "          -0.7090,  0.4457],\n",
      "         [-0.4897, -1.8688, -0.8012,  0.5150, -0.0738, -1.3466, -0.7090,\n",
      "           0.4457, -1.2756],\n",
      "         [-1.8688, -0.8012,  1.3305, -0.0738, -1.3466,  1.1518,  0.4457,\n",
      "          -1.2756, -0.7034],\n",
      "         [ 0.8573,  0.5150, -0.0738,  0.2045, -0.7090,  0.4457, -0.3336,\n",
      "          -0.4023,  2.3146],\n",
      "         [ 0.5150, -0.0738, -1.3466, -0.7090,  0.4457, -1.2756, -0.4023,\n",
      "           2.3146,  0.1836],\n",
      "         [-0.0738, -1.3466,  1.1518,  0.4457, -1.2756, -0.7034,  2.3146,\n",
      "           0.1836, -0.8412],\n",
      "         [ 0.2045, -0.7090,  0.4457, -0.3336, -0.4023,  2.3146,  0.9116,\n",
      "           1.2257, -1.0256],\n",
      "         [-0.7090,  0.4457, -1.2756, -0.4023,  2.3146,  0.1836,  1.2257,\n",
      "          -1.0256, -0.3944],\n",
      "         [ 0.4457, -1.2756, -0.7034,  2.3146,  0.1836, -0.8412, -1.0256,\n",
      "          -0.3944, -0.5429]],\n",
      "\n",
      "        [[ 0.5863, -0.1345, -0.3005, -0.2621,  0.9806,  0.1233,  1.2416,\n",
      "          -1.0087, -0.1818],\n",
      "         [-0.1345, -0.3005,  0.0024,  0.9806,  0.1233, -0.8503, -1.0087,\n",
      "          -0.1818, -0.3855],\n",
      "         [-0.3005,  0.0024, -0.3322,  0.1233, -0.8503, -0.3742, -0.1818,\n",
      "          -0.3855,  0.9696],\n",
      "         [-0.2621,  0.9806,  0.1233,  1.2416, -1.0087, -0.1818, -0.7441,\n",
      "          -0.4278,  0.1171],\n",
      "         [ 0.9806,  0.1233, -0.8503, -1.0087, -0.1818, -0.3855, -0.4278,\n",
      "           0.1171,  1.1782],\n",
      "         [ 0.1233, -0.8503, -0.3742, -0.1818, -0.3855,  0.9696,  0.1171,\n",
      "           1.1782, -1.6178],\n",
      "         [ 1.2416, -1.0087, -0.1818, -0.7441, -0.4278,  0.1171, -0.7069,\n",
      "          -0.5989, -0.0132],\n",
      "         [-1.0087, -0.1818, -0.3855, -0.4278,  0.1171,  1.1782, -0.5989,\n",
      "          -0.0132,  0.7005],\n",
      "         [-0.1818, -0.3855,  0.9696,  0.1171,  1.1782, -1.6178, -0.0132,\n",
      "           0.7005,  0.7016]]])\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 0\n",
    "im2col_result = F.unfold(images, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "print(\"im2col result\")\n",
    "print(im2col_result.shape)\n",
    "print(im2col_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernels Flattened\n",
      "torch.Size([4, 9])\n",
      "tensor([[-0.5057,  0.3242, -0.9349, -2.7210,  0.4371, -2.2433, -0.5837, -1.0012,\n",
      "         -0.1248],\n",
      "        [ 0.8013, -0.6438, -0.2755, -0.3278,  0.1188,  0.7968,  0.6063,  1.2937,\n",
      "          1.3797],\n",
      "        [ 0.1598,  0.3949,  1.2515,  0.4712, -1.0708,  0.4329, -0.5294,  0.3866,\n",
      "         -1.2594],\n",
      "        [ 0.4063, -1.6015, -1.7729, -2.5825,  3.4356,  1.2719,  1.8954,  0.1855,\n",
      "         -0.0976]])\n"
     ]
    }
   ],
   "source": [
    "out_channels = 4\n",
    "conv_kernel = torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "kernel_matrix = conv_kernel.view(out_channels, -1)\n",
    "print(\"Kernels Flattened\")\n",
    "print(kernel_matrix.shape)\n",
    "print(kernel_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution using im2col (Unfold)\n",
      "torch.Size([2, 4, 9])\n",
      "tensor([[[  0.9300,   2.1048,  -2.4255,  -1.7551,   3.8547,  -3.0984,  -6.8418,\n",
      "            3.7506,  -3.2507],\n",
      "         [ -0.8135,  -1.8323,  -2.9167,   3.0502,   3.1038,   0.1128,   3.1267,\n",
      "           -1.0784,  -1.9162],\n",
      "         [ -3.8917,   0.0741,   3.1126,  -1.5974,  -2.1191,   2.0743,   2.8688,\n",
      "           -4.6708,   0.2914],\n",
      "         [  3.1699,  -0.2183,  -4.1291,  -3.6754,   4.1035,  -1.8403,   4.9070,\n",
      "           12.6554,  -4.9095]],\n",
      "\n",
      "        [[  1.1141,   0.0807,   0.9668,  -2.2282,   3.8544,  -2.8831,   1.8041,\n",
      "           -0.3410,   2.1049],\n",
      "         [  0.1368,  -2.2779,   0.1383,  -2.3899,   2.4601,   0.8989,   0.7590,\n",
      "            1.0941,   0.5142],\n",
      "         [ -2.2740,   0.7740,  -0.9300,   2.1671,  -2.5184,   2.3932,  -0.1100,\n",
      "           -1.1025,  -1.4804],\n",
      "         [  7.3730,  -4.6759,  -3.7635, -10.2997,   2.2944,   3.0523,   1.5935,\n",
      "            2.3644,   0.5488]]])\n"
     ]
    }
   ],
   "source": [
    "im2col_conv2d_result = torch.matmul(kernel_matrix, im2col_result)\n",
    "print(\"Convolution using im2col (Unfold)\")\n",
    "print(im2col_conv2d_result.shape)\n",
    "print(im2col_conv2d_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape im2col multiplication result into output feature map\n",
      "torch.Size([2, 4, 3, 3])\n",
      "tensor([[[[  0.9300,   2.1048,  -2.4255],\n",
      "          [ -1.7551,   3.8547,  -3.0984],\n",
      "          [ -6.8418,   3.7506,  -3.2507]],\n",
      "\n",
      "         [[ -0.8135,  -1.8323,  -2.9167],\n",
      "          [  3.0502,   3.1038,   0.1128],\n",
      "          [  3.1267,  -1.0784,  -1.9162]],\n",
      "\n",
      "         [[ -3.8917,   0.0741,   3.1126],\n",
      "          [ -1.5974,  -2.1191,   2.0743],\n",
      "          [  2.8688,  -4.6708,   0.2914]],\n",
      "\n",
      "         [[  3.1699,  -0.2183,  -4.1291],\n",
      "          [ -3.6754,   4.1035,  -1.8403],\n",
      "          [  4.9070,  12.6554,  -4.9095]]],\n",
      "\n",
      "\n",
      "        [[[  1.1141,   0.0807,   0.9668],\n",
      "          [ -2.2282,   3.8544,  -2.8831],\n",
      "          [  1.8041,  -0.3410,   2.1049]],\n",
      "\n",
      "         [[  0.1368,  -2.2779,   0.1383],\n",
      "          [ -2.3899,   2.4601,   0.8989],\n",
      "          [  0.7590,   1.0941,   0.5142]],\n",
      "\n",
      "         [[ -2.2740,   0.7740,  -0.9300],\n",
      "          [  2.1671,  -2.5184,   2.3932],\n",
      "          [ -0.1100,  -1.1025,  -1.4804]],\n",
      "\n",
      "         [[  7.3730,  -4.6759,  -3.7635],\n",
      "          [-10.2997,   2.2944,   3.0523],\n",
      "          [  1.5935,   2.3644,   0.5488]]]])\n"
     ]
    }
   ],
   "source": [
    "out_height = (height - kernel_size) // stride + 1\n",
    "out_width = (width - kernel_size) // stride + 1\n",
    "output_feature_map = im2col_conv2d_result.view(batch_size, out_channels, out_height, out_width)\n",
    "print(\"Reshape im2col multiplication result into output feature map\")\n",
    "print(output_feature_map.shape)\n",
    "print(output_feature_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution2d call result\n",
      "torch.Size([2, 4, 3, 3])\n",
      "tensor([[[[  0.9300,   2.1048,  -2.4255],\n",
      "          [ -1.7551,   3.8547,  -3.0984],\n",
      "          [ -6.8418,   3.7506,  -3.2507]],\n",
      "\n",
      "         [[ -0.8135,  -1.8323,  -2.9167],\n",
      "          [  3.0502,   3.1038,   0.1128],\n",
      "          [  3.1267,  -1.0784,  -1.9162]],\n",
      "\n",
      "         [[ -3.8917,   0.0741,   3.1126],\n",
      "          [ -1.5974,  -2.1191,   2.0743],\n",
      "          [  2.8688,  -4.6708,   0.2914]],\n",
      "\n",
      "         [[  3.1699,  -0.2183,  -4.1291],\n",
      "          [ -3.6754,   4.1035,  -1.8403],\n",
      "          [  4.9070,  12.6554,  -4.9095]]],\n",
      "\n",
      "\n",
      "        [[[  1.1141,   0.0807,   0.9668],\n",
      "          [ -2.2282,   3.8544,  -2.8831],\n",
      "          [  1.8041,  -0.3410,   2.1049]],\n",
      "\n",
      "         [[  0.1368,  -2.2779,   0.1383],\n",
      "          [ -2.3899,   2.4601,   0.8989],\n",
      "          [  0.7590,   1.0941,   0.5142]],\n",
      "\n",
      "         [[ -2.2740,   0.7740,  -0.9300],\n",
      "          [  2.1671,  -2.5184,   2.3932],\n",
      "          [ -0.1100,  -1.1025,  -1.4804]],\n",
      "\n",
      "         [[  7.3730,  -4.6759,  -3.7635],\n",
      "          [-10.2998,   2.2944,   3.0523],\n",
      "          [  1.5935,   2.3644,   0.5488]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
    "conv_layer.weight.data = conv_kernel\n",
    "\n",
    "conv2d_result = conv_layer(images)\n",
    "conv2d_no_grad = conv2d_result.detach()\n",
    "print(\"Convolution2d call result\")\n",
    "print(conv2d_no_grad.shape)\n",
    "print(conv2d_no_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.7881e-07,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  2.3842e-07],\n",
      "          [ 0.0000e+00,  0.0000e+00, -4.7684e-07]],\n",
      "\n",
      "         [[ 0.0000e+00,  1.1921e-07,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  7.4506e-08],\n",
      "          [ 0.0000e+00, -1.1921e-07, -1.1921e-07]],\n",
      "\n",
      "         [[ 0.0000e+00,  9.6858e-08,  2.3842e-07],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -4.7684e-07, -2.9802e-08]],\n",
      "\n",
      "         [[ 0.0000e+00, -2.6822e-07, -4.7684e-07],\n",
      "          [ 0.0000e+00,  0.0000e+00, -5.9605e-07],\n",
      "          [ 0.0000e+00, -9.5367e-07, -4.7684e-07]]],\n",
      "\n",
      "\n",
      "        [[[-1.1921e-07, -7.4506e-09, -1.1921e-07],\n",
      "          [ 0.0000e+00,  4.7684e-07,  2.3842e-07],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 5.9605e-08,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00, -2.3842e-07, -1.1921e-07],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00, -5.9605e-08, -5.9605e-08],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [-8.1956e-08,  0.0000e+00,  1.1921e-07]],\n",
      "\n",
      "         [[ 0.0000e+00,  4.7684e-07, -2.3842e-07],\n",
      "          [-9.5367e-07,  2.3842e-07, -2.3842e-07],\n",
      "          [-2.3842e-07,  0.0000e+00, -1.7881e-07]]]])\n"
     ]
    }
   ],
   "source": [
    "errors = conv2d_no_grad - output_feature_map\n",
    "print(errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
